{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\") # ADD YOUR TOKEN HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from Hugging Face Hub\n",
    "dataset_id = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
    "dataset = load_dataset(dataset_id, split=\"train\")\n",
    "# select a random subset of 50k samples\n",
    "dataset = dataset.shuffle(seed=42).select(range(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_thinking_level(dataset):\n",
    "    dataset = dataset.map(lambda x: {\"thinking_level\": random.randint(1, 10)})\n",
    "    return dataset\n",
    "\n",
    "dataset = add_thinking_level(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['target', 'nums', 'thinking_level'],\n",
       "    num_rows: 50000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load tokenizer from Hugging Face Hub to format the dataset to our \"r1\" prompt \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")\n",
    "\n",
    "# gemerate r1 prompt with a prefix for the model to already start with the thinking process\n",
    "def generate_r1_prompt(numbers, target):\n",
    "    r1_prefix = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.\"\n",
    "      },\n",
    "      { \n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 = 1 </answer>.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Let me solve this step by step.\\n<think>\"\n",
    "      }]\n",
    "    return {\"prompt\": tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=True), \"target\": target}\n",
    "\n",
    "\n",
    "\n",
    "def generate_r1_prompt_v2(numbers, target, thinking_level):\n",
    "      r1_prefix = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.\"\n",
    "      },\n",
    "      { \n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Using the numbers {numbers}, create an equation that equals {target}, thinking level is {thinking_level}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 = 1 </answer>. The magnitude of thinking is controlled by a thinking level from 1 - 10, where 1 is no thinking and 100 is large thinking time i.e if you are given level is 1 then you should only think for 1 or 2 sentences and if you given level is 10 then you can think for lot more sentence and perform a longer reasoning for your answer.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Let me solve this step by step.\\n<think>\"\n",
    "      }]\n",
    "      return {\"prompt\": tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=True), \"target\": [target, thinking_level]}\n",
    "\n",
    "# convert our dataset to the r1 prompt\n",
    "dataset = dataset.map(lambda x: generate_r1_prompt_v2(x[\"nums\"], x[\"target\"], x[\"thinking_level\"]))\n",
    "\n",
    "# split the dataset into train and test\n",
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def format_reward_func(completions, target, **kwargs):\n",
    "    \"\"\"\n",
    "    Format: <think>...</think><answer>...</answer>\n",
    "    Args:\n",
    "        completions (list[str]): Generated outputs\n",
    "        target (list[str]): Expected answers\n",
    "      \n",
    "      Returns:\n",
    "          list[float]: Reward scores\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for completion, gt, tk_level in zip(completions, target):\n",
    "\n",
    "      try:\n",
    "        # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n",
    "        completion = \"<think>\" + completion        \n",
    "        # Check if the format is correct\n",
    "        regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
    "\n",
    "        match = re.search(regex, completion, re.DOTALL) \n",
    "        # if the format is not correct, reward is 0\n",
    "        if match is None or len(match.groups()) != 2:\n",
    "            rewards.append(0.0)\n",
    "        else:\n",
    "            rewards.append(1.0)\n",
    "      except Exception:\n",
    "        rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "def format_reward_func_2(completions, target, thinking_level, **kwargs):\n",
    "    \"\"\"\n",
    "    Format: <think>...</think><answer>...</answer>\n",
    "    \n",
    "    Additionally, this reward function checks that the length of the <think> section \n",
    "    (measured by the number of sentences) roughly matches the provided thinking level.\n",
    "    \n",
    "    For a thinking_level of 1, we expect 1 or 2 sentences; for a higher level (>=2), \n",
    "    the <think> section must contain at least as many sentences as the thinking level.\n",
    "    \n",
    "    Args:\n",
    "        completions (list[str]): Generated outputs.\n",
    "        target (list[str]): Expected answers.\n",
    "        thinking_level (list[int]): List of integers indicating the expected thinking level\n",
    "                                    which controls the required reasoning length.\n",
    "      \n",
    "    Returns:\n",
    "        list[float]: Reward scores (1.0 if both formatting and thinking length match standards, else 0.0).\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "\n",
    "    for completion, gt, tk_level in zip(completions, target, thinking_level):\n",
    "        try:\n",
    "            # Prepend synthetic <think> tag (as it is prefilled in the prompt)\n",
    "            completion = \"<think>\" + completion        \n",
    "            # Check if the overall format is correct using a regex\n",
    "            regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
    "            match = re.search(regex, completion, re.DOTALL) \n",
    "            \n",
    "            # If the format is incorrect, reward is 0.\n",
    "            if match is None or len(match.groups()) != 2:\n",
    "                rewards.append(0.0)\n",
    "            else:\n",
    "                think_text = match.group(1).strip()\n",
    "                # Split the think_text into sentences.\n",
    "                # This approach splits on punctuation marks (., !, or ?) that are followed by whitespace (or end-of-string).\n",
    "                sentences = [s for s in re.split(r'(?<=[.!?])\\s+', think_text) if s.strip()]\n",
    "                sentence_count = len(sentences)\n",
    "                \n",
    "                # Check the sentence count against the provided thinking level.\n",
    "                if tk_level == 1:\n",
    "                    # If thinking level is 1, expect 1 or 2 sentences.\n",
    "                    if sentence_count in (1, 2):\n",
    "                        rewards.append(1.0)\n",
    "                    else:\n",
    "                        rewards.append(0.0)\n",
    "                else:\n",
    "                    # For levels >= 2, require at least a number of sentences equal to the thinking level.\n",
    "                    if sentence_count >=  tk_level - 2:\n",
    "                        rewards.append(1.0)\n",
    "                    else:\n",
    "                        rewards.append(0.0)\n",
    "        except Exception:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "def equation_reward_func(completions, target, nums, **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluates completions based on:\n",
    "    2. Mathematical correctness of the answer\n",
    "\n",
    "    Args:\n",
    "        completions (list[str]): Generated outputs\n",
    "        target (list[str]): Expected answers\n",
    "        nums (list[str]): Available numbers\n",
    "    \n",
    "    Returns:\n",
    "        list[float]: Reward scores\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion, gt, numbers in zip(completions, target, nums):\n",
    "      try:\n",
    "        # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n",
    "        completion = \"<think>\" + completion\n",
    "        # Check if the format is correct\n",
    "        match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n",
    "        if match is None:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        # Extract the \"answer\" part from the completion\n",
    "        equation = match.group(1).strip()\n",
    "        # Extract all numbers from the equation\n",
    "        used_numbers = [int(n) for n in re.findall(r'\\d+', equation)]\n",
    "        \n",
    "        # Check if all numbers are used exactly once\n",
    "        if sorted(used_numbers) != sorted(numbers):\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace\n",
    "        allowed_pattern = r'^[\\d+\\-*/().\\s]+$'\n",
    "        if not re.match(allowed_pattern, equation):\n",
    "           rewards.append(0.0)\n",
    "           continue\n",
    "        \n",
    "        # Evaluate the equation with restricted globals and locals\n",
    "        result = eval(equation, {\"__builti'ns__\": None}, {})\n",
    "        # Check if the equation is correct and matches the ground truth\n",
    "        if abs(float(result) - float(gt)) < 1e-5:\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "      except Exception:\n",
    "            # If evaluation fails, reward is 0\n",
    "            rewards.append(0.0) \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_sample_1 = \"\"\"We need to find an equation using the numbers 19, 36, 55, and 7\n",
    "# exactly once, with basic arithmetic operations, that equals 65. One possible\n",
    "# combination is 55 + 36 - 19 + 7... </think>\n",
    "# <answer> 55 + 36 - 7 - 19 </answer>\"\"\"\n",
    "\n",
    "# thinking_level = 1 \n",
    "\n",
    "# correct_sample_2 = \"\"\" ... </think>\n",
    "# <answer> 55 + 36 - 7 - 19 </answer>\"\"\"\n",
    "\n",
    "# wrong_format = \"\"\"User: Using the numbers [19, 36, 55, 7], create an equation that equals 65.\"\"\"\n",
    "\n",
    "# wrong_format_2 = \"\"\"To find the equation that equals 79 using the numbers 95, 78, 6, 88, I'll start by adding 88 and 95:                      \n",
    "# 95 + 88 = 183                                                                                                              \n",
    "# Now, let's subtract 104 from 183 to get 79:\n",
    "# 183 - 104 = 79\n",
    "# <think> 183 - 104 = 79 </think><think> 183 - 104 = 79 </think><answer> 183 - 104 = 79 </answer>\"\"\"\n",
    "\n",
    "# wrong_result = \"\"\" ... </think>\n",
    "# <answer> 55 + 36 - 7 - 18 </answer>\"\"\"\n",
    "\n",
    "\n",
    "# test_rewards = format_reward_func(completions=[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result], target=[\"65\", \"65\", \"65\", \"65\", \"65\"], nums=[[19, 36, 55, 7]] * 5)\n",
    "# assert test_rewards == [1.0, 1.0, 0.0, 0.0, 1.0], \"Reward function is not working\"\n",
    "# test_rewards = equation_reward_func(completions=[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result], target=[\"65\", \"65\", \"65\", \"65\", \"65\"], nums=[[19, 36, 55, 7]] * 5)\n",
    "# assert test_rewards == [1.0, 1.0, 0.0, 0.0, 0.0], \"Reward function is not working\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-08 21:46:29,098] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-08 21:46:34 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41bdf30168b48168e7dbf35a7907be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2829f2dde944d7b9c9884edaef7a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1290b01dca8a44f8bccfeca960bb9139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4facdb9ead34d26aca8bd28157e490c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f48ab500210f43b899c4d148ef44abf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1d072adaea463da3e8fdf114f610ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b83af0df4848478c7dc53d483b3337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig\n",
    "\n",
    "# our model we are going to use as policy \n",
    "model_config = ModelConfig(\n",
    "    model_name_or_path=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    use_peft=True,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"qwen-r1-aha-moment\",\n",
    "    learning_rate=5e-7,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    bf16=True,\n",
    "    # GRPO specific parameters\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=1024, # max length of the generated output for our solution\n",
    "    num_generations=2,\n",
    "    beta=0.001,\n",
    "    \n",
    ")\n",
    "trainer = GRPOTrainer(\n",
    "    model=model_config.model_name_or_path,\n",
    "    reward_funcs=[format_reward_func_2, equation_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=get_peft_config(model_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/GRPOtraining/testing.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B141.148.185.79/home/ubuntu/GRPOtraining/testing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train and push the model to the Hub\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B141.148.185.79/home/ubuntu/GRPOtraining/testing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B141.148.185.79/home/ubuntu/GRPOtraining/testing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Save model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B141.148.185.79/home/ubuntu/GRPOtraining/testing.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model(training_args\u001b[39m.\u001b[39moutput_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Train and push the model to the Hub\n",
    "trainer.train()\n",
    "# Save model\n",
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
